# -*- coding: utf-8 -*-
"""Artical_ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZMynhM1J4VfDXwq4leh3bljg4XHomAz9
"""

!pip install unstructured
!pip install langchain-ollama
!pip install langchain_community
!pip install faiss-cpu
!pip install sentence_transformers
!pip install gradio

!pip install gradio

import os
import pickle
import time
import langchain
import transformers
from transformers import pipeline
from langchain.chains import LLMChain
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

os.environ["HUGGINGFACEHUB_API_KEY"] = ""

"""Reciving the data from URLS"""

loaders = UnstructuredURLLoader(urls=[
    "https://science.nasa.gov/universe/black-holes/",
    "https://en.wikipedia.org/wiki/Black_hole",
    "https://www.space.com/15421-black-holes-facts-formation-discovery-sdcmp.html",
    "https://www.nasa.gov/mission_pages/chandra/mysteries/black-hole-facts-myths.html"
])
data = loaders.load()
len(data)

"""Transfoming the data into Chunks"""

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200
)
docs = text_splitter.split_documents(data)
len(docs)

docs[0]

embedding = HuggingFaceEmbeddings()

vector_index = FAISS.from_documents(docs, embedding)

file_path = "vector_index.pkl"
with open(file_path,"wb") as f:
  pickle.dump(vector_index,f)

"""Importing LLM Model"""

model = pipeline(task="text2text-generation", model="google/flan-t5-large",max_new_tokens=500)

llm = HuggingFacePipeline(pipeline=model)

from langchain.chains import StuffDocumentsChain
from langchain.prompts import PromptTemplate

prompt_template = """Use the following pieces of context to answer the question at the end.
If you don't know the answer, just say that you don't know, don't try to make up an answer.
Provide a detailed, comprehensive explanation using as much relevant information from the context as possible.
Your answer should be at least several paragraphs long and cover all important aspects of the question.


Context: {context}
Question: {question}
Answer:"""
prompt = PromptTemplate(
    template=prompt_template, input_variables=["context", "question"]
)

llm_chain = LLMChain(llm=llm,prompt=prompt)

stuff_chain = load_qa_with_sources_chain(
    llm=llm,
    chain_type="map_reduce"
)

chain = RetrievalQAWithSourcesChain(
    retriever=vector_index.as_retriever(search_kwargs={"k": 1}),
    combine_documents_chain=stuff_chain
)

ques="what is a blackhole"

langchain.debug=True

result = chain({"question":ques})
print(f"Answer: {result['sources']}")

print(f"Answer: {result['sources']}")

# Add this code to the end of your existing script

import gradio as gr
import os
import pickle
import time
import langchain
import transformers
from transformers import pipeline
from langchain.chains import StuffDocumentsChain
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain import HuggingFacePipeline
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.qa_with_sources.loading import load_qa_with_sources_chain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import UnstructuredURLLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS


with open("vector_index.pkl", "rb") as f:
    vector_index = pickle.load(f)

def answer_question(question):
    if not question.strip():
        return "Please enter a question."

    try:
        prompt_template = """Use the following pieces of context to answer the question at the end.
        If you don't know the answer, just say that you don't know, don't try to make up an answer.
        Provide a detailed, comprehensive explanation using as much relevant information from the context as possible.
        Your answer should be at least several paragraphs long and cover all important aspects of the question.

        Context: {context}
        Question: {question}
        Answer:"""
        prompt = PromptTemplate(
            template=prompt_template, input_variables=["context", "question"]
        )

        model = pipeline(task="text2text-generation", model="google/flan-t5-large", max_new_tokens=500)
        llm = HuggingFacePipeline(pipeline=model)

        stuff_chain = load_qa_with_sources_chain(
              llm=llm,
              chain_type="map_reduce"
          )

        chain = RetrievalQAWithSourcesChain(
              retriever=vector_index.as_retriever(search_kwargs={"k": 5}),
              combine_documents_chain=stuff_chain
          )

        langchain.debug = True

        result = chain({"question": question})
        answer = result.get("sources", "No answer found.")

        return answer

    except Exception as e:
        return f"Error answering question: {str(e)}"

# Create Gradio interface
with gr.Blocks(theme=gr.themes.Base()) as demo:
    gr.Markdown("# Article Research Tool ðŸ”­")

    with gr.Row():
        with gr.Column():
            gr.Markdown("""### Ask Questions About Black Holes

This tool uses data from NASA and Wikipedia to answer your questions about black holes.

Example questions:
- What is a black hole?
- How do black holes form?
- What happens inside a black hole?
- What is Hawking radiation?
            """)

            question_input = gr.Textbox(
                label="Question",
                placeholder="Ask a question about black holes...",
                lines=2
            )
            submit_btn = gr.Button("Submit Question", variant="primary")

    with gr.Row():
        answer_output = gr.Markdown("<h2>Answer will appear here...</h2>", label="Answer")  # Bigger font size

    # Set up event handlers
    def process_answer(question):
        answer = answer_question(question)
        return f"<h2>{answer}</h2>"
    # Set up event handlers
    submit_btn.click(fn=process_answer, inputs=question_input, outputs=answer_output)
    question_input.submit(fn=process_answer, inputs=question_input, outputs=answer_output)

# Launch the application
demo.launch(share=True)  # share=True makes it accessible via a public URL in Colab